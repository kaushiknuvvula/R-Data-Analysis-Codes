---
title: "Year Level Topic Modeling of Movies over 100 Years"
author: "Kaushik Nuvvula"
date: "December 19, 2015"
output: html_document
self_contained: yes
---

#Part 2 - Year level movie genre analysis

```{r}
#libraries used
set.seed(1000)
library(xlsx)
library(dplyr)
library(tm)
library(ggplot2)
library(lsa)
library(scatterplot3d)
library(SnowballC)
library(NLP)
library(RColorBrewer)
library(wordcloud)
library(topicmodels)
library(reshape2)
library(stats)
library(RColorBrewer)
#install.packages("RColorBrewer")




set.seed(1000)
# year level analysis
year_data <- read.csv("movieTagline.csv",header = TRUE)

#Splitting the data into each year and saving each file as a text document
year_data = tbl_df(year_data)

for ( m in 1915:2014){ 
  year_data1=filter(year_data, year== m) 
  year_data1=select(year_data1,c(4))
  write.table(year_data1, file =sprintf("%s.txt",m),row.names=FALSE)}

#Specifying the directory name and creating a corpus
dirname <- file.path("C:/Users/Kaushik Nuvvula/Desktop/EDA/Final_Exam/", "year_files")
docs <- Corpus(DirSource(dirname, encoding = "UTF-8"))
meta(docs[[1]])

# The following steps pre-process the raw text documents.

# 1) Remove punctuations and numbers because they are generally uninformative.
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)

# 2) Convert all words to lowercase.
docs <- tm_map(docs, content_transformer(tolower))

# 3) Remove stopwords such as "a", "the", etc.
docs <- tm_map(docs, removeWords, stopwords("english"))

# 4) Use the SnowballC package to do stemming.
#install.packages("SnowballC")
docs <- tm_map(docs, stemDocument)

# 5) Remove excess white spaces between words.
docs <- tm_map(docs, stripWhitespace)


# Inspect the first document to see what it looks like.
docs[[1]]$content

# Lets visualize the entire corpus

pal <- brewer.pal(8, "Dark2")

# Inspect the words in 1915 document to see what it looks like.
#from the word bubble we see words like success, poster, origin more frequently
wordcloud(docs[1], min.freq=1, max.words = 200, random.order = TRUE, col = pal )


# Inspect the words in 2014 document to see what it looks like.
#from the word bubble we see words like love, time, world  adventure kill more frequently
wordcloud(docs[100], min.freq=2, max.words = 200, random.order = TRUE, col = pal )


# Convert all documents to a term frequency matrix.
tfm <- DocumentTermMatrix(docs)

# We can check the dimension of this matrix by calling dim()
print(dim(tfm))

# Use topicmodels package to conduct LDA analysis.
# As we got good topic split for k=7 in our decade  level analysis, I choose k=7 in this case as well.

set.seed(1000)
results <- LDA(tfm, k = 7, method = "Gibbs")

# Obtain the top ten words (i.e., the 10 most probable words) for each topic.
Terms <- terms(results, 10)
Terms

# Obtain the most likely topic assignment for each document.
Topic <- topics(results, 1)
Topic

# Get the posterior probability for each document over each topic
posterior <- posterior(results)[[2]]
posterior 

#look at the posterior topic distribution for the first document and plot it visually
posterior(results)[[2]][1,]
barplot(posterior(results)[[2]][1,])

# Appending year column to the probability table
year = c(1915:2014)

posterior1 = cbind(posterior, year)

posterior_sort <- posterior1[order(year),]
posterior_sort
#posterior_sort1 = posterior_sort[, 1:10]

#GGplot across years
year_plot = as.data.frame.matrix(posterior_sort) 

control1 <- melt(year_plot, id.var="year",variable.name = "topic",value.name = "proportion"  )


#Assigning topic names using match and index
index <- c(1, 2, 3, 4,5, 6,7)
values <- c("Thrillers/Action", "Cult/Seasonal", "Science Fiction", "Drama", "Theatrical","Horror/Terror","Romance/Fiction")

control1$topicname <- values[match(control1$topic, index)] 

#Let us have a look at the melted dataframe
control1

#Here is the stacked bar plot 
library(ggplot2)
ggplot(control1, aes(x = year, y = proportion, fill = topicname)) + 
  geom_bar(stat = "identity")  +labs(title="Movie Genres Across each year using Gibbs LDA - Clear Topics (Advised)")

#Alternately lets visualize using line graph can show the trend clearly
ggplot(control1, aes(x = year, y = proportion, colour = topicname)) + 
  geom_line(stat = "identity", size = 1.2 ) +labs(title="Movie Genres Across each year using Gibbs LDA - Clear Topics (Advised)")

#examine the main topic for document 1
# In a similar way we can examine topics for multiple documents
Terms[,7]

# Appendix trying LDA with VEM
# I tried using variational expectation maximization (VEM)
#Howver as we see, from the plots the topics are not well split across this data set
# Hence, I  prefer using gibbs method on this data set rather than VEM
set.seed(1000)
results_VEM <- LDA(tfm, k = 7, method = "VEM")

# Obtain the top ten words (i.e., the 10 most probable words) for each topic.
Terms <- terms(results_VEM, 10)
Terms

# Obtain the most likely topic assignment for each document.
Topic <- topics(results_VEM, 1)
Topic

# Get the posterior probability for each document over each topic
posterior <- posterior(results_VEM)[[2]]
posterior 

#look at the posterior topic distribution for the first document and plot it visually
posterior(results_VEM)[[2]][1,]
barplot(posterior(results_VEM)[[2]][1,])

# Appending year column to the probability table
year = c(1915:2014)

posterior1 = cbind(posterior, year)

posterior_sort <- posterior1[order(year),]
posterior_sort
#posterior_sort1 = posterior_sort[, 1:10]

#GGplot across years
year_plot = as.data.frame.matrix(posterior_sort) 

control1 <- melt(year_plot, id.var="year",variable.name = "topic",value.name = "proportion"  )


#Assigning topic names using match and index
index <- c(1, 2, 3, 4, 5, 6, 7)
values <- c("Romance", "Love/Fiction", "Love", "Thrill", "Life","World","Insipirational")
control1$topicname <- values[match(control1$topic, index)]

#Let us have a look at the melted dataframe
control1

#Here is the stacked bar plot 
library(ggplot2)
ggplot(control1, aes(x = year, y = proportion, fill = topicname)) + 
  geom_bar(stat = "identity")  +labs(title="Movie Genres Across each year using VEM (LDA) - No clear topics (not advised)")

# Even here we see that VEM (variational expectation maximization) is not able to split the genres clearly
#as we see the 0.99 proportions in the stacked bar chart. 
#Hence, I would suggest using Gibbs LDA for the year level analysis as well.

```
